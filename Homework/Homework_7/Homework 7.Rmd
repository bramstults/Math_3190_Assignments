---
title: "MATH 3190 Homework 7"
author: 'Focus: Notes 9'
date: "Due April 6, 2024"
output:
  html_document:
    df_print: paged
  pdf_document: default
header-includes: \usepackage{multirow}
editor_options:
  chunk_output_type: console
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, fig.width = 6, fig.height = 4)
options(width = 55)
library(tidyverse)
library(RColorBrewer)
library(ggplot2)
library(caret)
library(dplyr)
library(httr)
library(readxl)
library(car)
library(glmnet)
library(kableExtra)
library(pls)
```

Your homework should be completed in R Markdown or Quarto and Knitted to an html or pdf document. You will \`\`turn in" this homework by uploading to your GitHub Math_3190_Assignment repository in the Homework directory.

Some of the parts in this homework, like 1a, 2a, 2b, and 2c require writing down some math-heavy expressions. You may either type it up using LaTeX style formatting in R Markdown, or you can write it by hand (neatly) and include pictures or scans of your work in your R Markdown document.

# Problem 1 (33 points)

Suppose we want to find the value of $x$ and the objective function value for finding the global maximum and the minimum of the function $f(x)=\ln(1+x^2)(1+x)e^{-x^2}$. We will implement gradient descent and Newton's method for finding these extrema.

### Part a (4 points)

Find $f'(x)$ for this function. Do this by hand as a nice derivative refresher. Feel free to check your answer using something like Wolfram Alpha.

**Response:** Where the triple product $fgh$ is defined, then $\frac{d}{dx}fgh = f'gh+fg'h+fgh'$, and so:

$$
f'(x)=\dfrac{2x(x+1)e^{-x^2}}{x^2+1}+e^{-x^2}\ln(x^2+1)(-2x^2-2x+1)
$$

### Part b (12 points)

Write a function that implements gradient descent. Use a backtracking line search each iteration to find $\gamma_k$: the step size. This function should take eight inputs:

-   The starting value (or vector)
-   The function to optimize
-   The function's derivative (or gradient)
-   A logical indicating if we want to find a max with a default of `FALSE`.
-   The maximum number of iterations with a default of 200
-   The initial step size with a default of 1
-   The line search beta parameter with a default of 0.5.
-   The stopping tolerance with a default of `1e-10`.

The output of this function should be a list with the $x$ value (or vector) that optimizes the function, the function value at that point, and the number of iterations it took to converge. This function should work in the case of one dimension or multiple dimensions since you'll use it again in problem 2.

```{r prob1b, eval=T, echo=T}


gradient_descent <- function(x_k, f, gradient, findmax = FALSE, max_iter = 200,
                             gamma = 1, beta = 0.5, tolerance = 1e-10) {
  
  if (findmax) {
    obj_func <- function(x) -f(x)
    obj_gradient <- function(x) -gradient(x)
    } else {
      obj_func <- f
      obj_gradient <- gradient
    }
  
  for (k in 1:max_iter) {
    
    
    f_eval <- f(x_k)
    
    #  line search
    while (obj_func(x_k - gamma * obj_gradient(x_k)) >
           obj_func(x_k) - gamma/4 * sum(obj_gradient(x_k) ^ 2) ) {
      gamma <- beta * gamma
    }
    
    # Update x_k
    x_new <- x_k - gamma * obj_gradient(x_k)
    
    # Check convergence
    if ( norm(x_k - x_new, type = '2') < tolerance) {
      break
    }
    
    x_k <- x_new
  }
  
  results <- c(x_k, f_eval, k)
  
  if(findmax){
    print(paste("The maximum is:", f_eval))
    print("Max at x_k / betas :")
    print(x_k)
  }else{
    print(paste("The minimum is:", f_eval))
    print("Min at x_k / beta:")
    print(x_k)
  }
  
  print(paste("with", k, "iterations"))
  
}


```

### Part c (3 points)

Use your gradient descent function to find the global **min** of $f(x)=\ln(1+x^2)(1+x)e^{-x^2}$. Try using several starting points. Keep track of and report the number of iterations needed to converge at the different starting points.

```{r prob1c, eval=T, echo=T}

fun <- function(x){
  f_x <- log(1+x^2)*(1+x)*exp(-x^2)
}

fun_prime <- function(x){
  (2*x*(x+1)*exp(-x^2)) / (x^2 + 1) + exp(-x^2)*log(x^2 + 1)*(-2*x^2 - 2*x + 1)
}


gradient_descent(x_k= 1, f=fun, gradient=fun_prime, findmax = F, gamma=1, beta=0.5)
gradient_descent(x_k= 0.5, f=fun, gradient=fun_prime, findmax = F, gamma=1, beta=0.5)
gradient_descent(x_k= -0.5, f=fun, gradient=fun_prime, findmax = F, gamma=1, beta=0.5)

gradient_descent(x_k= -1, f=fun, gradient=fun_prime, findmax = F, gamma=1, beta=0.5)

```

**Response:** The minimum of -0.0623 is obtained in 31 iterations. The minimization becomes easily stuck around 0 for $x_k=-0.5, 0.5$, converging in seven to eight iterations to $0$, whereas it does not converge for $x_k=1$

### Part d (3 points)
Use your gradient descent function to find the global **max** of $f(x)=\ln(1+x^2)(1+x)e^{-x^2}$. Again, try several starting points. Keep track of and report the number of iterations needed to converge at the different starting points.

```{r prob1d, eval=T, echo=T}

x_k_minus1_max <- gradient_descent(x_k=-1 , f=fun, gradient=fun_prime, findmax = T, gamma=0.9, beta=0.5)
x_k_minus.5_max <- gradient_descent(x_k=-0.5 , f=fun, gradient=fun_prime, findmax = T, gamma=0.9, beta=0.5)
x_k_.5_max <- gradient_descent(x_k=0.5 , f=fun, gradient=fun_prime, findmax = T, gamma=0.9, beta=0.5)
x_k_1_max <- gradient_descent(x_k=1 , f=fun, gradient=fun_prime, findmax = T, gamma=0.9, beta=0.5)


```

**Response:** The maximum of $0.5102$ is obtained in five iterations obtained by initializing $x_k=1$. The maximization seems to become easily stuck in particular for negative values, maximizing at $0.0879$ for $x_k=-1, -0.5$.

### Part e (6 points)

Use the fact that $$
f''(x)=\dfrac{2e^{-x^2}}{(1+x^2)^2}\left((2x^3+2x^2-3x-1)(1+x^2)^2\ln(1+x^2)-4x^5-4x^4-3x^3-5x^2+3x+1\right)
$$ to implement Newton's method to find the global max **and** the global min. Use the same starting points you did in parts c and d. Keep track of and report the number of iterations needed to converge at the different starting points. And yes, that second derivative is very messy! This is one reason why Newton's method is less popular. You don't have to write a function for this part to implement Newton's method, but you can if you'd like.

```{r prob1e, eval=T, echo=T}

fun_second <- function(x){
  (2*exp(-x^2)/(1+x^2)^2) * ((2*x^3 +2*x^2 -3*x -1) * (1+x^2)^2 
                             * log(1+x^2) -4*x^5 -4*x^4 -3*x^3 -5*x^2 +3*x +1)
}


# for(k in 1:maxit){
#   x_new <- x_k - fun_prime(x_k)/fun_second(x_k)
#   if(abs(x_k-x_new) < 1e-10){
#     break
#   }
#   x_k <- x_new
# }

newtons_method <- function(x_k,f, f_prime, f_second, findmax = FALSE, 
                           max_iter =200, tolerance = 1e-10) {
  
  # objective function and gradient based on findmax
  if (findmax) {
    f_prime_2 <- function(x) -f_prime(x)
    f_second_2 <- function(x) -f_second(x)
  } else {
    f_prime_2 <- function(x) f_prime(x)
    f_second_2 <- function(x) f_second(x)
  }
  
  #  newton's groove
  
  for (k in 1:max_iter) {
    
    f_eval <- f(x_k)
    
    x_new <- x_k - f_prime_2(x_k)/f_second_2(x_k)
    
    if( abs(x_k - x_new) < 1e-10){
      break
    }
    x_k <- x_new
  }
  results <- c(x_k, f_eval, k)
  
  if(findmax){
    print("The maximum is:")
    print(f_eval)
  }else{
    print("The minimum is:")
    print(f_eval)
    }
  print(results)  
}


###

newtons_method(x_k=-1,f=fun, f_prime=fun_prime, f_second=fun_second, 
               findmax = F, max_iter = 200, tolerance = 1e-10) 
newtons_method(x_k=-1.03,f=fun, f_prime=fun_prime, f_second=fun_second, 
               findmax = F, max_iter = 200, tolerance = 1e-10)
  
newtons_method(x_k=1, f=fun, f_prime=fun_prime, f_second=fun_second, 
               findmax = T, max_iter = 200, tolerance = 1e-10) 

```

### Part f (3 points)

Compare the number of iterations needed for convergence for gradient descent and for Newton's method. **Response:** Line backtracking with gradient descent allowed to converge on the minimum in 31 iterations and converged on the maximum in 1 iteration given the nearby $x_k=1$. Newton's method repeatedly failed to converge on the minimum without a very nearby initializing argument, $1.03\leq x_k$, whereas the maximum was converged upon in 6 iterations.

### Part g (2 points)

Use the `optimize()` function in **R** to find the global max and min.

```{r prob1g, eval=T, echo=T}

opt <- optimize(fun,c(-100,100))
print(paste('Minimum at x=', opt$minimum))
print(paste('y=',opt$objective))


opt <- optimize(fun,c(-100,100), maximum=T)
print(paste('Maximum at x=', opt$maximum))
print(paste('y=',opt$objective))

```

# Problem 2 (27 points)

We implemented optimization to maximize the likelihood for a Poisson regression problem in the notes to estimate the $\beta_i$ values for $i=0,\dots,p-1$. Let's do something similar to find the $\boldsymbol{\beta}$ vector by maximizing the likelihood in logistic regression.

In logistic regression, we attempt to predict the probability of a success for a given case. We can use Bernoulli random variable for this. For a Bernoulli random variable, $P(Y=y_i) = p_i^{y_i}(1-p_i)^{1-y_i}$ for $y_i=0,1$ where $p_i$ is the probability of a success. In our case, for logistic regression, we have $\text{logit}(p_i)=\mathbf{X}_i\boldsymbol{\beta}$. That means $p_i = \dfrac{\exp(\mathbf{X}_i\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i\boldsymbol{\beta})}$.

### Part a (4 points)

Find the likelihood function, $L(\boldsymbol{\beta}|\mathbf{X},\boldsymbol{y})$, for the logistic regression for a sample of size $n$.

![Problem 1a response](/Users/bram/Documents/Math_3190/Math_3190_Assignments/Homework/Homework_7/3190_HW7_Prob2-a.pdf){#id.class width="75%"}

### Part b (6 points)

Show that the log likelihood function is $$
\ell(\boldsymbol{\beta}|\mathbf{X},\boldsymbol{y})=\sum_{i=1}^ny_i\ln\left(\dfrac{\exp(\mathbf{X}_i\boldsymbol{\beta})}{1+\exp(\mathbf{X}_i\boldsymbol{\beta})}\right) + \sum_{i=1}^n(1-y_i)\ln\left(\dfrac{1}{1+\exp(\mathbf{X}_i\boldsymbol{\beta})}\right)
$$ and then show it can be equivalently written

$$
\ell(\boldsymbol{\beta}|\mathbf{X},\boldsymbol{y}) = \sum_{i=1}^ny_i(\mathbf{X}_i\boldsymbol{\beta}) - \sum_{i=1}^n\ln\Big(1 + \exp(\mathbf{X}_i\boldsymbol{\beta})\Big).
$$

![Problem 1b response](/Users/bram/Documents/Math_3190/Math_3190_Assignments/Homework/Homework_7/3190_HW7_Prob2-b.pdf){#id.class width="75%"}

### Part c (5 points)

Find the gradient of the log likelihood if we have three $\beta$'s. That is, if $\mathbf{X}_i\boldsymbol{\beta}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}$.

![Problem 1b response](/Users/bram/Documents/Math_3190/Math_3190_Assignments/Homework/Homework_7/3190_HW7_Prob2-c.pdf){#id.class width="75%"}

### Part d (3 points)

In Homework 3, Problem 2, part a, you read in the `adult` dataset (from the UC Irvine [database](https://archive.ics.uci.edu/dataset/2/adult)). This is the one that we used to predict whether a person makes over \$50K a year based on some other variables. The data came from the Census Bureau in 1994 and can be found in the Data folder in my Math3190_S24 GitHub repo. More info on the dataset can be found in the "adult.names" file.

```{r prob2d, eval=T, echo=T}

response <- GET("https://raw.githubusercontent.com/rbrown53/Math3190_Sp24/main/Data/adult.data")

adult_data <- readr::read_csv(content(response, as = "text"),col_names = FALSE) |> 
  rename(age = X1,
         work_class = X2,
         final_wght = X3,
         education = X4,
         edu_num = X5,
         marital = X6,
         occupation = X7,
         relationship = X8,
         race = X9,
         sex = X10,
         capital_gain = X11,
         capital_loss = X12,
         hrs_per_week = X13,
         country_origin = X14,
         salary = X15) |> 
  mutate(salary = as_factor(salary),
         work_class = as_factor(work_class),
         education = as_factor(education),
         marital = as_factor(marital),
         occupation = as_factor(occupation),
         relationship = as_factor(relationship),
         race = as_factor(race),
         sex = factor(sex, level =c("Female", "Male")),
         country_origin = as_factor(country_origin))


head(adult_data)

```

Read in the dataset and put column names like you did in HW 3. Then fit a logistic regression model using the `glm()` function that predicts salary from age/10 and sex. 
Note: we should divide the age by 10 in our model because this makes the gradient descent more stable (for whatever reason). 
We can divide by 10 in the `glm()` function by wrapping it in the `I()` function. Like this: `glm(salary ~ I(age/10) + sex, data = adult, family = binomial)`.

```{r prob2d2, eval=T, echo=T}


adult_logit_mod <- glm(salary ~ I(age/10) + sex, 
                       data = adult_data, family = binomial)

summary(adult_logit_mod)
```


### Part e (2 points)

Define `y`, `x1`, and `x2`. `y` should be a vector of 0's and 1's with a 1 indicating that the person had a salary above \$50,000. `x1` should be a vector that contains all of the age values (divided by 10) and then `x2` should be an indicator variable that indicates if the person is male or not. Taking columns from the matrix obtained using the `model.matrix()` may be useful here.

```{r prob2e, eval=T, echo=T}

design_matrix <- model.matrix(adult_logit_mod)

y <- ifelse(adult_data$salary == ">50K", 1, 0)

x_1 <- design_matrix[,2]

x_2 <- design_matrix[,3]


```


### Part f (5 points)

Use your gradient descent function from problem 1 to find estimates for $\beta_0$, $\beta_1$, and $\beta_2$ for predicting salary from age and sex. Enter the following in your function:

-   Use the vector `c(0, 0, 0)` as your starting point.
-   Instead of starting the step size, $\gamma_k$, at 1, start it at 0.1 to save some time with the line search.
-   Make sure the maximum number of iterations is at least 200.

You'll know you did this right if the optimized values you end up with match (or are very close to) the estimates from your logistic regression model you fit in part d. This may take a minute or two to run.

```{r prob2f, eval=T, echo=T}

start <- c(0,0,0)

# syntax for matrix multip. would be function(b, X, y){ sum(y * X %*% b)} etc.

ln_likely <- function(b){
  sum( y * (b[1] + b[2]*x_1 + b[3]*x_2) ) - sum( log( 1 + exp(b[1] + b[2]*x_1 + b[3]*x_2)) )
}

ln_likely_2 <- function(b){
  #exp_line <- exp(b[1] + b[2]*x_1 + b[3]*x_2)
  sum(y * log((exp(b[1] + b[2]*x_1 + b[3]*x_2))/(1+exp(b[1] + b[2]*x_1 + b[3]*x_2))) )+sum((1-y)*log(1/(1+exp(b[1] + b[2]*x_1 + b[3]*x_2))))
}
 
nabla_likely <- function(b){
  
  #exp_values <- exp(b[1] + b[2]*x_1 + b[3]*x_2)
  #exp_values <- pmax(exp_values, .Machine$double.eps) # clipping to avoid underflow
  
  d_beta_0 <- sum(y - (exp(b[1] + b[2]*x_1 + b[3]*x_2)) / (1 + exp(b[1] + b[2]*x_1 + b[3]*x_2)))
  
  d_beta_1 <- sum(y * x_1 - (x_1 * exp(b[1] + b[2]*x_1 + b[3]*x_2) ) / (1 + exp(b[1] + b[2]*x_1 + b[3]*x_2)))
  
  d_beta_2 <- sum(y * x_2 - (x_2 * exp(b[1] + b[2]*x_1 + b[3]*x_2) ) / (1 + exp(b[1] + b[2]*x_1 + b[3]*x_2)))
  
  c(d_beta_0, d_beta_1, d_beta_2)
}

gradient_descent( x_k= start, f= ln_likely, gradient= nabla_likely, findmax = T, 
                 gamma=0.1 , beta=0.5 , max_iter = 200)


```

### Part g (2 points)

Use the `optim()` function in **R** to find the estimate for the $\beta$'s here. This should be much faster since this function is much better optimized (no pun intended). The results here should also be very close to the numbers we obtained the slope in the model we fit in part d, but may not match exactly.

```{r}

max_likely_optimal <- optim(par=start, fn=ln_likely, gr=nabla_likely)

max_likely_optimal$par


```

