---
title: "MATH 3190 Homework 4"
author: 'Focus: Notes 7 Part 1'
date: "Due March 9, 2024"
output:
  html_document:
    df_print: paged
header-includes: \usepackage{multirow}
editor_options:
  chunk_output_type: console
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, fig.width = 6, fig.height = 4)
options(width = 55)
library(tidyverse)
library(RColorBrewer)
library(ggplot2)
library(caret)
library(dplyr)
library(httr)
library(readxl)
library(car)
library(glmnet)
library(kableExtra)
```

Your homework should be completed in R Markdown or Quarto and Knitted to an html or pdf document. You will ``turn in" this homework by uploading to your GitHub Math_3190_Assignment repository in the Homework directory.

# Problem 1 (55 points)

Concrete is the most important material in civil engineering. The concrete compressive strength is an important attribute of the concrete and our goal is to predict the concrete compressive strength (in MPa) from the following variables:

* Cement (in $\text{kg}/\text{m}^3$)
* Blast furnace slag (in $\text{kg}/\text{m}^3$)
* Fly ash (in $\text{kg}/\text{m}^3$)
* Water (in $\text{kg}/\text{m}^3$)
* Superplasticizer (in $\text{kg}/\text{m}^3$)
* Coarse aggregate (in $\text{kg}/\text{m}^3$)
* Fine aggregate (in $\text{kg}/\text{m}^3$)
* Age of concrete (in days)

This dataset came from the [UCI ML Repository](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength) and can be found on the Math3190_Sp24 GitHub page along with a Readme file.

### Part a (3 points)
Read in the dataset. The data file is a .xls file, so you'll need to either convert it to a .csv or, _preferably_, use the `readxl` package to read in the Excel file. Once it is read in, change the names of the variables so they are shorter yet still descriptive. 

```{r prob1a, eval=T, echo=T}

concrete_data <- readxl::read_excel( '/Users/bram/Documents/Math_3190/Homework_4_Data/Concrete_Data.xls' )|> 
  rename(cement = 'Cement (component 1)(kg in a m^3 mixture)',
         furnSlag = 'Blast Furnace Slag (component 2)(kg in a m^3 mixture)',
         flyAsh = 'Fly Ash (component 3)(kg in a m^3 mixture)',
         water = 'Water  (component 4)(kg in a m^3 mixture)',
         superplast = 'Superplasticizer (component 5)(kg in a m^3 mixture)',
         coarseAgg = 'Coarse Aggregate  (component 6)(kg in a m^3 mixture)',
         fineAgg = 'Fine Aggregate (component 7)(kg in a m^3 mixture)',
         ageDays = 'Age (day)',
         compression = 'Concrete compressive strength(MPa, megapascals)'
         )
head(concrete_data)
```

### Part b (5 points)
In the `GGally` library is the function `ggpairs`, which makes a nice scatterplot matrix in the `ggplot2` style. Create this scatterplot matrix for all of the variables in the dataset (they should all be plotted together in one plot). 

Comment on the scatterplot matrix. Which variables seem to have a (at least somewhat) linear relationship with compressive strength? Does it seem like multicollinearity will be an issue here? Does it seem like the transformation of at least one variable is appropriate? There should be one (fairly) obvious variable that needs to be transformed.

```{r prob1b, eval=T, echo=T, cache=T}

library(GGally)

pair_plot <- ggpairs(concrete_data)

pair_plot

```
**Response:** The cement component of concrete is the most correlated with compression strength with a positive correlation of 0.498. The next two most compression correlated components are superplasticizer and the age in days of the concrete.  Regarding the distribution plot of `Age (Days)`, there does appear to be a highly skewed distribution with high density in the lower range and much less in the upper range, and thus may benefit from a log-transformation.  Other candidates are `Blast Furnace Slag`, `Fly Ash` and `Superplasticizer`.

### Part c (8 points)
Fit a linear model (with the `lm()` function) predicting compressive strength using all other variables and include any transformations you thought were appropriate in part b. 

```{r prob1c1, eval=T, echo=T}
concrete_data_2 <- concrete_data |> 
  mutate(ageDays = log(ageDays))

linear_model <- lm(compression ~ . , data=concrete_data_2)

summary(linear_model)
```

Using `ggplot()`, make a QQ plot of the raw residuals. Include the QQ line as well. Comment on whether the residuals appear to be approximately normal.

```{r prob1c2, eval=T, echo=T}
##Normal QQ plot

raw_residuals <- as.data.frame(residuals(linear_model))
names(raw_residuals) <- "residuals"

ggplot(raw_residuals, aes(sample = residuals)) +
  stat_qq() +  
  stat_qq_line(color = "brown") +  
  labs(title = "QQ Plot of Raw Residuals", x = "Theoretical Quantiles", 
       y = "Sample Quantiles")

```
**Response:** This appears to have relatively normal residuals although some quantiles may be much larger quantities than expected, in particular in the upper quantiles where the residuals appear to depart from normal distribution... may be skewed right.

Using `ggplot()`, make a plot of the jackknife residuals (obtained using the `rstudent()` function) on the y-axis and the fitted values of the model on the x-axis. Comment on whether this residual plot looks good. If it does not, indicate what the problem(s) is (are).

```{r prob1c3, eval=T, echo=T}

##Jackknife residuals and plot 
jackknife_1 <- rstudent(linear_model)

ggplot(linear_model, aes(x = .fitted, y = jackknife_1)) +
  geom_point(color = "slategray4") +
  geom_hline(yintercept=0, color = "brown", lwd=1) +
  labs(title = "Jackknife Residuals", x = "Fitted Values", 
       y = "Studentized Residuals") 

```
**Response:** There appears to be some degree of heteroskedasticity regarding the intense clustering and fan-shape in the spread of residual data.


### Part d (6 points)
Let's do some model selection to determine if any variables should be dropped from the model. 

First, use the `step()` function on the model you fit in the previous part. This will select the variables using AIC. 

```{r prob1d1, eval= TRUE, echo= TRUE}

step(linear_model,direction="backward")
#seems like all the coefficients

```

Second, use `step()` with the option `k` equal to the log of the sample size. This will select the variables using BIC.

```{r, prob1d2, eval=T, echo=T}

step(linear_model, scope=formula(linear_model), k=log(nrow(concrete_data_2)) )

#removes superplasticizer

```

Third, use the `cv.glmnet()` function in the `glmnet` library (set a seed first) to fit a LASSO for variable selection. 

Note, the `model.matrix()` function will be helpful here to get a matrix to input for the `x` argument in the `cv.glmnet()` function. Use the "lambda.1se" value to select the variables and using that $\lambda$ value, fit the LASSO model using `glmnet()`. 

```{r, prob1d3 , eval=T, echo=T}

set.seed(2024)
predictors <- model.matrix(linear_model)[,-1]
response <- concrete_data_2$compression

# LASSO model using cv.glmnet()
lasso_model <- cv.glmnet(predictors, response, alpha = 1, nfolds=10)

plot(lasso_model)

best_lambda <- lasso_model$lambda.1se #as per instructions .1se used for minimum

lasso_model_2 <- glmnet(predictors, response, alpha = 1, lambda = best_lambda)

# coefficients of the best model
best_coef <- coef(lasso_model_2)
#seems to remove coarseAgg and fineAgg (?)


# selected variables (coefficients not exactly zero)
selected_vars <- names(best_coef[abs(best_coef[,1]) > 1e-6])
cat("Selected variables:", paste(selected_vars, collapse = ", "))
#nichts zurückgegeben

print(best_coef)
```
Finally, compare the variables that were selected by the three methods. 

### Part e (4 points)
Using the variables selected by the method that reduced the number of variables the most, fit a new ordinary least squares (OLS) model for predicting compressive strength using the `lm()` function.

```{r, prob1e1 , eval=T, echo=T}

linear_model_2 <- concrete_data_2 |>  
  select(compression, cement, furnSlag, flyAsh, water, superplast, ageDays) |> 
  lm(compression ~ . , data = _ )

```

Using `ggplot()`, make a QQ plot of the raw residuals. Include the QQ line as well. Comment on whether the residuals appear to be approximately normal and whether this plot looks better than the QQ plot in part c.

```{r prob1e2, eval=T, echo=T}
##Normal QQ plot

raw_residuals_2 <- as.data.frame(residuals(linear_model_2))
names(raw_residuals_2) <- "residuals"

ggplot(raw_residuals_2, aes(sample = residuals)) +
  stat_qq() +  
  stat_qq_line(color = "brown") +  
  labs(title = "2nd QQ Plot of Raw Residuals", x = "Theoretical Quantiles", 
       y = "Sample Quantiles")

```
**Response:** This plot appears to be a somewhat more normal distribution of residual data than what the previous QQ-plot presented, but follows the pattern of the precedent with the form of the tails.

Using `ggplot()`, make a plot of the jackknife residuals (obtained using the `rstudent()` function) on the y-axis and the fitted values of the model on the x-axis. Comment on whether this residual plot looks good and whether this plot looks better than the residual plot in part c. If it does not, indicate what the problem(s) is (are).

```{r prob1e3, eval=T, echo=T}

##Jackknife residuals and plot 
jackknife_2 <- rstudent(linear_model_2)
ggplot(linear_model_2, aes(x = .fitted, y = jackknife_2)) +
  geom_point(color = "slategray4") +
  geom_hline(yintercept=0, color = "brown", lwd=1) +
  labs(title = "2nd Model Jackknife Residuals", x = "Fitted Values", 
       y = "Studentized Residuals") 
```
**Response:** The model obtained subsequent to the LASSO-based reduction of variables presents a a distribution of residual data which is equally if not more heteroskedastic with a clearly fan-shaped scatterplot and unequally dense clusters.

### Part f (10 points)
Since the residual plot still does not look good, let's try to use weighted least squares. Following slides 12-14 of Notes 7, create a vector of weights and then fit a model using weighted least squares. 

```{r prob1f1, eval=T, echo=T}

error_model <- lm( abs(linear_model_2$residuals) ~ linear_model_2$fitted.values)

w <- 1/error_model$fitted.values^2

linear_model_w <- concrete_data_2 |>  
  select(compression, cement, furnSlag, flyAsh, water, superplast, ageDays) |> 
  lm(compression~. , data= _ , weights = w )

summary(linear_model_w) #remember: R^2 has essentially no meaning here

```

Using `ggplot()`, make a plot of the jackknife residuals (obtained using the `rstudent()` function) on the y-axis and the fitted values of the weighted model on the x-axis. Comment on whether this residual plot looks good and whether this plot looks better than the residual plot in part e.

```{r, prob1f2, eval=T, echo=T}

jackknife_w <- rstudent(linear_model_w)

ggplot(linear_model_w, aes(x=.fitted, y=jackknife_w)) +
  geom_point(color = "slategray4") +
  geom_hline(yintercept=0, color = "brown", lwd=1) +
  labs(title = "Weighted Model Jackknife Residuals", x = "Fitted Values", 
       y = "Studentized Residuals") 



```
**Response:** There is marked improvement in apparent in the scatterplot of residual data obtained from the weighted model.  The density of clustering has been moderated and the notable pattern symptomatic of heteroskedasticity been treated to avoid a discernable 'fan-shape'.  There is more or less equal repartition across the 0-line.


### Part g (9 points)
Using the unweighted model from part e and the weighted model from part f, find and report both a confidence interval for the mean compressive strength and a prediction interval for the specific compressive strength for concrete that has a cement value of 300, a blast furnace slag of 90, a fly ash of 50, a water value of 200, a superplasticizer of 2.5, a coarse aggregate of 900, a fine aggregate of 600, and an age of 300 days. 
Note: some of those variables will not be used since you reduced the number of variables earlier. You can use the `predict()` function here and remember that you will need to find the specific weight for the given predictor variable values for the weighted intervals.

Comment on how these intervals differ. Does the change make sense given the value of the fitted value?

```{r prob1gConf, echo=T, eval=T}

row_obs <- data.frame(cement = 300, furnSlag = 90, flyAsh = 50, water = 200, 
                      superplast = 2.5, ageDays = log(300) )

#OLS
conf_interval_ols <- predict(linear_model_2, row_obs , interval='confidence' )

#WLS
conf_interval_wls <- predict(linear_model_w, row_obs , interval='confidence' ) 

#OLS
pred_interval_ols <- predict(linear_model_2, row_obs , interval='prediction' )

#WLS
pred_interval_wls <- predict(linear_model_w, row_obs , interval='prediction' ) 

print(kable(conf_interval_ols, caption = "OLS Confidence Interval",
           col.names = c("Fit", "Lower Bound", "Upper Bound"),
           booktabs = TRUE, 
           align = 'c'))

print(kable(conf_interval_wls, caption = "Weighted Confidence Interval",
           col.names = c("Fit", "Lower Bound", "Upper Bound"),
           booktabs = TRUE, 
           align = 'c'))

print(kable(pred_interval_ols, caption = "OLS Prediction Interval",
           col.names = c("Fit", "Lower Bound", "Upper Bound"),
           booktabs = TRUE, 
           align = 'c'))

print(kable(pred_interval_wls, caption = "Weighted Prediction Interval",
           col.names = c("Fit", "Lower Bound", "Upper Bound"),
           booktabs = TRUE, 
           align = 'c'))


```
**Response:**
It can be seen that there is a slight difference of predicted outcome between OLS and Weighted LS, with the WLS prediction slightly lower.  The interval ranges reflect this difference.  The confidence interval ranges for OLS and WLS are very similar at around $2.23-2.24$ in width, and the change is reasonable given the different predictede values. Whereas the prediction intervals differ such that the WLS has tighter range of approximately $5.5$ versus OLS with range of approximately $28$. The interval difference for predicted values are less 'sensible' than for the confidence intervals.

### Part h (10 points)
Write a function called `predict_weighted` that takes three inputs: the **unweighted** model, the data frame containing the information about the value(s) of the predictor variables we are using to predict, and the interval type (either "confidence" or "prediction"). This function should return the predicted value and the interval bounds for the specified interval type for weighted least squares. So, this function should compute the weights, obtain the weighted least squares model, find the specific weight for the new value, and then get the prediction and the interval.
This function should work for any model you give it, not just for this exact situation of this problem. 
So, you should not reference any data sets or variables specific to this concrete problem in the function.

Test this new function for the confidence and prediction intervals you made in part g. 

```{r prob1hFunction, echo=T, echo=F}


predict_weighted <- function(model, data, observation, outcome, 
                             method = 'confidence'){
  
  if(!(method %in% c('confidence', 'prediction'))){
    return("Method must be either 'prediction' or 'confidence'")
  }
  if (!all(outcome %in% names(data))) {
    stop("Outcome variable not found in data")
  }
  ###
  e_mod <- lm( abs(model$residuals) ~ model$fitted.values )
  w <- 1/e_mod$fitted.values^2
  
  formula_string <- as.formula(paste(outcome, "~ ."))
  
  model_w <- lm( formula_string , data , weights = w )
  
  ###
  
  #confidence intervals
  if(method=='confidence'){
    #WLS
    conf_interval_wls <- predict(model_w, observation , interval='confidence' )
  } else if (method == 'prediction'){
    #WLS
    pred_interval_wls <- predict(model_w, observation , interval='prediction' )
  }
  ###
}

```

**Testing** function for confidence intervals:
```{r, prob1hTestConf}

concrete_data_test <- concrete_data_2 |> select(-fineAgg, -coarseAgg)

conf <- predict_weighted( model=linear_model_2, data=concrete_data_test, 
                          observation=row_obs, outcome='compression', 
                          method='confidence' )

print(conf)

```

**Testing** function for prediction intervals:
```{r, prob1hTest}

concrete_data_test <- concrete_data_2 |> select(-fineAgg, -coarseAgg)

pred <- predict_weighted( model=linear_model_2, data=concrete_data_test, 
                          observation=row_obs, outcome='compression', 
                          method='prediction' )

print(pred)

```


# Problem 2 (29 points)

An automobile consulting company wants to understand the factors on which the pricing of cars depends. The dataset `car_price_prediction.csv` in the GitHub data folder has information on the sales of 4340 vehicles. 

### Part a (3 points)
Read in the data file and take the log of all numeric variables. Then fit a linear model for predicting the log of selling price using all other variables except "name".
```{r prob2a, eval=T, echo=T}

car_prices_data <- readr::read_csv('/Users/bram/Documents/Math_3190/Homework_4_Data/car_price_prediction.csv') |> 
  select(-name) #just dropping `name` right off the bat

numeric_cols <- names(car_prices_data)[sapply(car_prices_data, is.numeric)]

car_prices <- car_prices_data
for (col in numeric_cols) {
  car_prices[, col] <- log(car_prices[, col])
}

head(car_prices)

car_linear_model <- lm(selling_price ~ . ,data=car_prices)
summary(car_prices)

```


### Part b (4 points)
Now fit some LASSO models for predicting log price using all but the "name" variable. Try the following values for the regularization parameter: $\lambda= 0.0, 0.01, 0.1,$ and $1$ and comment on how the coefficients of the model change. 

Note: when $\lambda=0$, the LASSO coefficients should equal the OLS model coefficients. However, they will actually be a bit off here. That is because the `glmnet()` function has a `thresh` argument that sets a threshold for convergence. It is, by default, set to `1e-07`. To make the parameters match, you can change that to `thresh = 1e-14` instead. This is not necessary, though. 
```{r prob2b, eval=T, echo=T}

car_price_predictors <- model.matrix(car_linear_model)[,-1] 
car_price_outcome <- car_prices$selling_price

lambda_values <- c(0, 0.01, 0.1, 1.0)

lasso_models <- list() #for fitted model storage

for (i in seq_along(lambda_values)) {
  lambda <- lambda_values[i]
  
  # Fit the LASSO model
  lasso_models[[i]] <- glmnet(car_price_predictors, car_price_outcome, 
                              alpha = 1, lambda = lambda, thresh = 1e-14)
}

#lasso_model_01 <- lasso_models[[2]] #just in case for model access
#coefficients_01 <- coef(lasso_model_01)

for (i in seq_along(lambda_values)) {
  cat("Coefficients for lambda =",lambda_values[i], ":\n")
  
  print( coef(lasso_models[[i]]) )
  
  print('===============================================')
}
```

### Part c (4 points)
Now fit some ridge regression models for predicting log price using all but name. Try the following values for the regularization parameter: $\lambda=0, 0.01, 0.1,$ and $1$ and comment on how the coefficients of the model change. 

```{r prob2c, eval=T, echo=T}

for (i in seq_along(lambda_values)) {
  lambda <- lambda_values[i]
  
  # Fit the LASSO model
  lasso_models[[i]] <- glmnet(car_price_predictors, car_price_outcome, 
                              alpha = 0, lambda = lambda, thresh = 1e-14)
}

#lasso_model_01 <- lasso_models[[2]] #just in case for model access
#coefficients_01 <- coef(lasso_model_01)

for (i in seq_along(lambda_values)) {
  cat("Coefficients for lambda =",lambda_values[i], ":\n")
  
  print( coef(lasso_models[[i]]) )
  
  print('===============================================')
}
```


### Part d (4 points)
Now fit some elastic net regression models for predicting log price using all but name. Try the following values for the regularization parameter: $\lambda=0, 0.01, 0.1,$ and $1$ for $\alpha=1/3$ and then comment on how the coefficients of the model change for each $\alpha$ of the LASSO, the Ridge, and the Elastic Net.

```{r, prob2d, eval=T, echo=T}

lambda_values <- c(0, 0.01, 0.1, 1.0)

lasso_models <- list() #for fitted model storage

for (i in seq_along(lambda_values)) {
  lambda <- lambda_values[i]
  
  # Fit the LASSO model
  lasso_models[[i]] <- glmnet(car_price_predictors, car_price_outcome, 
                              alpha = 1/3, lambda = lambda, thresh = 1e-14)
}

#lasso_model_01 <- lasso_models[[2]] #just in case for model access
#coefficients_01 <- coef(lasso_model_01)

for (i in seq_along(lambda_values)) {
  cat("Coefficients for lambda =",lambda_values[i], ":\n")
  
  print( coef(lasso_models[[i]]) )
  
  print('===============================================')
}


```
**Response:** The consecutive increasing lambdas for a LASSO regression $\alpha=1$ eliminate variable coefficients rapidly, reducing the magnitudes of those left, and ending on a intercept-only model for $\lambda=1$.  
The same range of lambdas in a Ridge regression $\alpha=0$ does not eliminate any variable coefficients, but reduces their magnitudes with each greater lambda.
The Elastic Net with $\alpha=\frac{1}{3}$ reduces magnitudes and eliminates variables, but at a more gradual rate than in LASSO, and does not end on an intercept-only model, but instead a model including an intercept of $-493.9$ and `year` coefficient of $66.6$.


### Part e (4 points)
Use the `cv.glmnet()` to find the "optimal" $\lambda$ value obtained by "lambda.1se" for 
LASSO, 
ridge, 
and elastic net 

and then fit models using the `glmnet()` function for all three using their respective "best" $\lambda$. 

**Set a seed** before running the `cv.glmnet()` each time. 

Compare each model's variables and coefficients. 

```{r prob2e, eval=T, echo=T}


alpha_values <- c(1, 0, 1/3)

cv_lasso_models <- list() #for fitted model storage
cv_lasso_model_best <- list()
best_coefs <- list() #for coefficients
best_lambda <- list() #lambdas

for (i in seq_along(alpha_values)) {
  alpha <- alpha_values[i]
  
  set.seed(2024)
  # Fit the LASSO model
  cv_lasso_models[[i]] <- cv.glmnet(car_price_predictors, car_price_outcome, 
                              alpha = alpha, nfolds=10, thresh = 1e-14)
  
  best_lambda[[i]] <- cv_lasso_models[[i]]$lambda.1se 
  #as per instructions .1se used for minimum
  
  cv_lasso_model_best[[i]] <- glmnet(car_price_predictors, car_price_outcome, 
                                 alpha = alpha, lambda = best_lambda[[i]])

# coefficients of the best model
best_coefs[[i]] <- coef(cv_lasso_model_best[[i]])
  
}

for (i in seq_along(alpha_values)) {
  
  cat("Best Lambda for alpha =",alpha_values[i], ":\n")
  print( best_lambda[[i]] )
  
  cat("Coefficients for alpha =",alpha_values[i], ":\n")
  print( coef(cv_lasso_model_best[[i]]) )
  print('===============================================')
}

```
**Response:** For the seeded LASSO regression, the optimized $\lambda=0.033$, the lowest value among the three $\alpha$'s.  This also returns a model with only six variables(`year`, `km_driven`, `fuelDiesel`, `Individual seller`, `Trustmark dealer` and `transmissionManual`) and the largest _absolute value_ of an intercept at $-1,722.44$.
For the seeded Ridge regression, the optimized $\lambda=0.148$ the highest value among the three.  This moreover returns all coefficients shrunk with the smallest intercept in _absolute value_ at  $-1,372.66$.
The Elastic Net regression with an $\alpha=\frac{1}{3}$ returns an optimized lambda between the others, nearer to the LASSO $\lambda$, with $\lambda=0.067$.  This model returns a model with most all variables, but has removed `fuelElectric`, `fuelLPG`,  `Test Drive Car` and has an intercept $=-1,608.07$

### Part f (10 points)
Use bootstrapping with at least 1000 samples to estimate the standard errors of the coefficients of the ridge regression model. For each bootstrap sample, run the `cv.glmnet()` function to find the best $\lambda$ and fit a model using that optimized $\lambda$. Use the "lambda.1se" for this. Then compare these bootstrapped standard errors to the standard errors for the OLS model you fit in part a. Are they larger or smaller? Does this make sense? 

```{r probf, eval=T, echo=T, cache=T}

n_boot <- 1000

coefficients <- matrix(0, nrow = n_boot, ncol=14) #1000 x 14 matrix

for (i in 1:n_boot){
  
  index <- sample(1:nrow(car_price_predictors), nrow(car_price_predictors), 
                  replace=T)  #car_price_predictors, car_price_outcome
  
  set.seed(2024)
  model <- cv.glmnet(car_price_predictors[index,], car_price_outcome[index], 
                     alpha=0)
  
  coefficients[i, ] <- coef(model, s='lambda.1se')[,1]
  
}

```

```{r}
 

#standard error = sigma/√n is an estimated standard deviation

#a standard deviation of a boots

bootstrap_sd_coefs <- apply(coefficients, 2, sd)

ols_se_coefs <- summary(car_linear_model)$coefficients[, 2]

names(bootstrap_sd_coefs) <- names(ols_se_coefs)

rounded_boot_sd <- round(bootstrap_sd_coefs, 6)
formatted_boot_sd <- format(rounded_boot_sd, scientific = FALSE)

rounded_ols_se_coefs<- round(ols_se_coefs, 6)

comparison <- data.frame(
  #Feature = names(bootstrap_sd_coefs),
  'Bootstrap SE' = formatted_boot_sd,
  'OLS SE' = rounded_ols_se_coefs
)

kable(comparison, format = 'markdown', 
      caption = 'Comparison of Standard Errors') |> 
  kable_styling(full_width = F, position = 'center')



```
**Response:** Considering the table, we see that the _standard errors_ of coefficients approximated by the bootstrap sampling of the ridge regression are much lower than those of the OLS coefficients for all variables and the intercept.  As the ridge regression has reduced the magnitude of coefficients of the model, the so-called 'shrinkage effect' of the L2-penalty, and thereby reduced the variance, _standard errors_ are lower and thus represent a more precise model.  

# Problem 3 (16 points)

Sixty districts in California in 1990 were randomly selected. We want to predict the median house value for the district based on the district location (longitude and latitude), the median house age in the block group, the total number of rooms in the homes, the total number of bedrooms, the population in the district, the number of households, and the median income (in \$10,000). The data are already in the .Rmd file.

```{r house_data, echo = F}
library(tidyverse) |> suppressPackageStartupMessages()
housing_train <- tibble(
  house_value = c(156100, 145300, 150000, 50600, 176300, 178500, 123800,
                  173900, 422700, 164400, 42100, 157800, 162900, 90500,
                  191800, 128100, 109700, 206700, 132200, 248200, 336900,
                  58000, 271500, 259500, 69400, 132700, 461200, 245000,
                  218000, 311700, 361600, 250000, 113900, 103600, 81900,
                  351900, 114100, 500001, 153300, 466700),
  longitude = c(-121.08, -121.27, -117.7, -119.29, -118.09, -117.67, 
                -120.76, -118.16, -118.39, -123.1, -117.66, -118.16, 
                -120.98, -120.99, -122.69, -118.18, -122.27, -122.11, 
                -121.99, -118.02, -118.02, -120.94, -118.6, -121.95, 
                -120.62, -118.18, -122.13, -117.93, -121.85, -118.49, 
                -122.44, -121.32, -117.07, -121.83, -121.43, -122.11, 
                -118.27, -118.4, -120.28, -117.16),
  latitude = c(38.95, 38.7, 34.06, 36.34, 33.9, 34.02, 38.47, 34.02,
               33.89, 38.79, 35.63, 33.97, 38.99, 37.67, 38.35, 33.91,
               37.83, 37.99, 38.34, 33.74, 33.73, 40.14, 34.26, 37.96, 
               36.99, 33.8, 37.4, 33.94, 36.6, 34.18, 37.76, 37.67, 32.74, 
               38, 38.53, 37.41, 34.01, 34.41, 37.9, 32.74),
  age = c(18, 16, 25, 28, 33, 16, 17, 44, 38, 20, 33, 23, 17, 28, 16, 41, 
          51, 16, 16, 26, 24, 31, 18, 18, 32, 42, 29, 30, 21, 31, 52, 21, 
          38, 15, 36, 27, 47, 22, 17, 43),
  rooms = c(1931, 3747, 2054, 1440, 3326, 3042, 1521, 1218, 1851, 3109, 
            2579, 1516, 3403, 1768, 1689, 1260, 2665, 3913, 1470, 3842, 
            6393, 3127, 6154, 2739, 2455, 2301, 6027, 2658, 2381, 3073, 
            2959, 1494, 1901, 6365, 2430, 5110, 921, 4443, 1047, 1437),
  bedrooms = c(380, 586, 609, 431, 720, 524, 309, 374, 332, 712, 564, 
               457, 661, 423, 254, 299, 574, 710, 261, 609, 1141, 664, 
               1070, 393, 508, 621, 1195, 382, 701, 674, 683, 271, 392, 
               1646, 426, 1599, 264, 560, 212, 406),
  population = c(1271, 1817, 2271, 2178, 2533, 1516, 607, 1175, 750, 1643,
                 1155, 1977, 1540, 1066, 921, 1535, 1258, 1782, 748, 1961,
                 2743, 1345, 3010, 1072, 1344, 2114, 2687, 1135, 1264, 1486,
                 1145, 781, 1099, 3838, 1199, 2764, 881, 1573, 530, 692),
  households = c(377, 590, 564, 440, 689, 475, 240, 342, 314, 638, 431, 435,
                 622, 392, 270, 322, 536, 676, 256, 595, 1057, 580, 1034, 374,
                 492, 561, 1171, 392, 659, 684, 666, 255, 406, 1458, 437,
                 1482, 221, 496, 196, 379)
)

housing_test <- tibble(
  house_value = c(214900, 44000, 194400, 128700, 435000, 179400, 129300,
                  312300, 76300, 225800, 295500, 162500, 340800, 93800,
                  155000, 283200, 133400, 248200, 265000, 57500),
  longitude = c(-119.25, -121.93, -122.31, -121.27, -118.42, -118.71, 
                -117.31, -122.04, -119.69, -117.88, -117.25, -121.48, 
                -122.49, -121.49, -116.18, -119.19, -118.23, -118.35, 
                -118.35, -124.15),
  latitude = c(34.26, 41.86, 38.27, 38.67, 33.85, 34.27, 34.02, 37.04, 
               36.79, 33.7, 32.8, 38.55, 37.74, 39.52, 33.69, 34.3, 33.89,
               34.28, 33.91, 40.88),
  age = c(30, 28, 34, 15, 43, 26, 18, 17, 15, 18, 26, 52, 48, 25, 17, 
          25, 16, 30, 31, 33),
  rooms = c(2948, 4225, 1748, 1701, 1584, 990, 1634, 4977, 2524, 2135, 
            2442, 2037, 1186, 848, 89, 2197, 5003, 3214, 2583, 2235),
  bedrooms = c(827, 835, 284, 346, 477, 223, 274, 994, 451, 373, 659, 
               358, 213, 153, 19, 320, 1180, 513, 663, 506),
  population = c(1635, 1908, 783, 723, 799, 719, 899, 1987, 1207, 1464, 
                 1134, 811, 487, 436, 79, 934, 4145, 1700, 1675, 1165),
  households = c(750, 686, 303, 352, 433, 232, 285, 947, 424, 405, 624, 
                 375, 207, 155, 21, 330, 1159, 533, 612, 441)
)
```

### Part a (2 points)
Fit a linear regression model predicting `house_value` from the other variables.

```{r prob3a, eval=T, echo=T}

haus_lineares_Modell <- lm(house_value ~ . ,data=housing_train)

summary(haus_lineares_Modell)

```

### Part b (4 points)
Fit a ridge regression using `cv.glmnet()` to choose the optimal $\lambda$. Set the seed before using`cv.glmnet()`.

```{r prob3b, eval=T, echo=T}

set.seed(2024)
Prädiktoren <- model.matrix(haus_lineares_Modell)[,-1]
Ergebnis <- housing_train$house_value

haus_anpassen_lineares_Modell <- cv.glmnet(x=Prädiktoren, y=Ergebnis, 
                                              alpha=0, nfolds=10)

beste_lambda <- haus_anpassen_lineares_Modell$lambda.1se

haus_angepasstes_lineares_Modell <- glmnet(x=Prädiktoren, y=Ergebnis, alpha = 0, 
                                           lambda = beste_lambda)

# Koeffizienten des besten Modells
beste_Koef <- coef(haus_angepasstes_lineares_Modell)

print(beste_Koef)
```

### Part c (6 points)
Compute the sum of squared errors for both the OLS model and the ridge regression model for the testing set. 
Remember to use the exact models you fit to the training sets. The `predict()` functions will be useful here. 
Compare the results and comment on why the ridge regression performs better here. Slides 26-29 of Notes 7 may be helpful interpreting why... variance vs. bias.
```{r prob3c, eval=T, echo=T}

#SSE = ∑ (y-hat - y)^2

###OLS
#predicted values OLS
vorhergesagten_Werte_ols <- predict(haus_lineares_Modell, 
          newdata = housing_test[, -which(names(housing_test) == "house_value")]
          )
#actual values from Test Data
tatsächlichen_Werte <- housing_test$house_value  

#squared errors OLS
quadratierten_Fehlern_ols <- 
  (tatsächlichen_Werte - vorhergesagten_Werte_ols)^2

SSE_ols <- sum(quadratierten_Fehlern_ols)


###Ridge
#for some reason predict() is requiring `newx` as an argument, thus defining :
neu_X <- housing_test[, -which(names(housing_test) == 'house_value')]

#predicted values Ridge
vorhergesagten_Werte_ridge <- predict(haus_angepasstes_lineares_Modell, 
                                      newx = as.matrix(neu_X))

quadratierten_Fehlern_ridge <- 
  (tatsächlichen_Werte - vorhergesagten_Werte_ridge)^2
SSE_ridge <- sum(quadratierten_Fehlern_ridge)

#Results
print(paste("Sum of Squared Errors (SSE) for OLS:", SSE_ols))
print(paste("Sum of Squared Errors (SSE) for Ridge with Optimal Lambda:", 
            SSE_ridge))
print(paste("Difference of OLS SEE and Ridge SSE:", 
            format((SSE_ols - SSE_ridge), scientific=TRUE)))

```
**Response:** Indeed the SSE is greater for the OLS model than for the Ridge regression.    The OLS regression performed only on training data has 'over-learned' the noise and variance of that set and is thereby less generalizable to the testing data. As the ridge regression has 'shrunk' coefficients, it has also reduced total variance 'learned' by the model and therefore lessens error over the testing set.

### Part d (4 points)
Use the `train()` function in the `caret()` package to find the optimal $\alpha, \lambda$ pair in this situation (using the training set and 5-fold cross validation). 

For $\lambda$, search for values between 1 and 10000 by 100. 

Then fit a model using the optimal $\alpha$ between 0 and 1 by 0.05 and $\lambda$ using `glmnet()` and compare the sum of squared errors using this optimized model to the two models in part c. 
Set a seed before running the `train()` function. 
```{r prob3d, eval=T, echo=T}

set.seed(2024)
  
train_haus_ridge <- train(house_value ~ . , method = 'glmnet',
                    data = housing_train,
                    trControl = trainControl(method ='cv', number=5),
                    tuneGrid = expand.grid(alpha = seq(0, 1, by = 0.05),
                                           lambda = seq(1, 10000, by = 100))
                   )
  
plot(train_haus_ridge)


#optimal alpha and lambda
optimal_alpha <- train_haus_ridge$bestTune$alpha
optimal_lambda <- train_haus_ridge$bestTune$lambda

#final model with optimal alpha and lambda
trained_haus_ridge <- glmnet(
  x = Prädiktoren,
  y = Ergebnis,
  alpha = optimal_alpha,
  lambda = optimal_lambda
)

haus_ridge_predicted_values <- predict(trained_haus_ridge, 
                                       newx = as.matrix(neu_X)) 
#where neu_X is the data from housing_test without `house_value` column

# Calculate squared errors
squared_errors_trained <- (tatsächlichen_Werte - haus_ridge_predicted_values)^2

# Calculate the sum of squared errors (SSE)
SSE_trained <- sum(squared_errors_trained)

# Print the SSE
print(paste("Sum of Squared Errors (SSE) for trained model:", SSE_trained))

```
**Response:** The SSE obtained from the trained model at optimized alpha and lambda values is again reduced in comparison to the OLS or even Ridge regression.  We see that from the OLS model, $SSE = 1.1127e+11$, from the L2/Ridge regression model $SSE = 1.0613e+11$ and from the trained model with optimal $\alpha=0.05$ and optimal $\lambda=3701$ the obtained $SSE = 8.3626e+10$ which is lower than the other models by an order of magnitude.

